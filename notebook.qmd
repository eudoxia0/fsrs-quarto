---
title: "FSRS"
format:
  html:
    code-fold: false
jupyter: python3
---

# Parameters

Retrievability $R$ is a real number in $[0, 1]$.

```{python}
R = float
```

Stability $S$ is a real number $[0, +\infty]$.

```{python}
S = float
```

Difficulty $D$ is a real number $[0, 10]$.

```{python}
D = float
```

Grade $G$ is one of:

- Forgot = 1
- Hard = 2
- Good = 3
- Easy = 4

```{python}
from enum import Enum

class Grade(Enum):
    FORGOT = 1
    HARD   = 2
    GOOD   = 3
    EASY   = 4
```

$w$ is a vector in $\mathbb{R}^{19}$.

```{python}
W: list[float] = [
    0.40255,
    1.18385,
    3.173,
    15.69105,
    7.1949,
    0.5345,
    1.4604,
    0.0046,
    1.54575,
    0.1192,
    1.01925,
    1.9395,
    0.11,
    0.29605,
    2.2698,
    0.2315,
    2.9898,
    0.51655,
    0.6621
]
```

# Retrievability

Retrievability is approximated by:

$$
R(t) = \left( 1 + F\frac{t}{S} \right)^C
$$

Where $t$ is time in days since the last review, and:

$$
\begin{align*}
F &= \frac{19}{81} \\
C &= -0.5
\end{align*}
$$

```{python}
T = float

F: float = 19.0/81.0
C: float = -0.5

def retrievability(t: T, s: S) -> R:
    return (1.0 + F*(t/s))**C
```

Retrievability curves for different values of stability:

```{python}
#| code-fold: true
import numpy as np
import matplotlib.pyplot as plt

def plot():
    t = np.linspace(0, 30, 200)
    s_values = [0.1, 0.5, 1.0, 2.0]
    for s in s_values:
        y = [retrievability(t_val, s) for t_val in t]
        plt.plot(t, y, label=f's = {s}')
    plt.xlabel('t')
    plt.ylabel('R(t)')
    plt.xlim(0, 30)
    plt.ylim(0.0, 1.0)
    plt.yticks(np.arange(0, 1.1, 0.1))
    plt.legend()
    plt.grid(True)
    plt.show()

plot()
```

# Interval

The review interval is:

$$
I = \frac{S}{F} \left( R_d^{(1/C)} - 1 \right)
$$

Where $R_d$ is the desired retention.

```{python}
def interval(rd: float, s: S) -> T:
    return (s/F)*(rd**(1.0/C) - 1.0)
```

Interval curves as a function of stability, for different values of $R_d$:

```{python}
#| code-fold: true

def plot():
    rd_curves = [0.9, 0.8, 0.7]
    for rd in rd_curves:
        s = np.linspace(0.1, 3.0, 100)
        i = [interval(rd, s_val) for s_val in s]
        plt.plot(s, i, label=f"$R_d$ = {rd}")

    plt.xlabel('S')
    plt.xlim(0.1, 3.0)
    plt.xticks(np.arange(0.0, 3.1, 0.5))

    plt.ylabel('I(S)')
    plt.ylim(0.0, 5.0)

    plt.legend()
    plt.grid(True)

    plt.show()

plot()
```

At higher $R_d$, reviews will be more frequent, which is what we expect.

Stability is defined as the interval where $R$ will equal $0.9$. So, for $R_d = 0.9$, $I(S) = S$ by definition:

```{python}
def eq(a: float, b: float) -> bool:
    return abs(a-b) < 0.001

for s in np.linspace(0.1, 3.0, 100):
    assert eq(interval(0.9, s), s)
```

# Updating Stability

This section describes how an item's stability is updated after a review.

## First Time

A card that has never been reviewed has no stability.

The first time the user reviews a card, its initial stability is:

$$
S_0(G) = w_{G-1}
$$

```{python}
def s_0(g: Grade):
    return W[g.value - 1]

assert s_0(Grade.FORGOT) == W[0]
assert s_0(Grade.HARD)   == W[1]
assert s_0(Grade.GOOD)   == W[2]
assert s_0(Grade.EASY)   == W[3]
```

That is, the parameters $w_0$ to $w_3$ represent the initial values of stability.

## Stability on Success

Stability is updated differently depending on whether the user forgot ($G=1$) or remembered ($G \in [2,3,4]$) the item. The equation is very big, so I'm going to break it down hierarchically.

After a review, stability is updated by multiplying it with a scaling factor $\alpha$:

$$
S'(D, S, R, G) = S\alpha
$$

Where:

$$
\alpha = 1 + t_d t_s t_r h(G) b(G) c
$$

The addition is because some of the multiplicative terms may be zero, and in that case, $\alpha=1$.

### $t_d$

This is the "difficulty penalty", defined by:

$$
t_d = 11-D
$$

Harder items (higher $D$) increase stability more slowly. The highest difficulty is $D=10$, here, $d=1$ and therefore difficulty provides no boost. This is intuitive: harder items are harder to consolidate.

### $t_s$

This determines how today's stability affects the next stability:

$$
t_s = S^{-w_9}
$$

If $S$ is high, updates will be smaller. The more stable a memory is, the harder it is to make it more stable. Memory stability saturates.

### $t_r$

This is about memory saturation:

$$
t_r = e^{w_{10}(1-R)} - 1
$$

If $R=1$ (100% recall) then $t_3=0$. So $\alpha$ as a whole is $1$, i.e. stability does not change. The lower $R$ is, the higher $\alpha$ will be. So the optimal time to review some material is when you have almost forgotten it. Which is somewhat counterintuitive, but it makes sense: the more you remember something, the fewer the gains from reviewing, dually, the more you have forgotten it, the more room there is to improve.

### $h$

This is the hard penalty:

$$
h(G) = \begin{cases}
  w_{15} & G = 2 \\
  1      & \text{otherwise}
\end{cases}
$$

If recall was hard, we apply $w_{15}$ (a learned parameter between 0 and 1). This penalizes stability growth where recall was shaky. Otherwise, it has no effect.

### $b$

This is the opposite of $h$, a bonus for easy recall:

$$
b(G) = \begin{cases}
  w_{16} & G = 4 \\
  1      & \text{otherwise}
\end{cases}
$$

If recall was easy, we multiply by $w_{16}$, a number greater than one, which scales stability up. Otherwise, it has no effect.

### $c$

Finally, $c$ just applies a learned parameter to control the shape of the curve:

$$
c = e^{w_8}
$$

Putting it all together:

```{python}
from math import exp

def s_success(d: D, s: S, r: R, g: Grade) -> S:
    t_d = 11 - d

    t_s = s ** (-W[9])

    t_r = exp(W[10] * (1.0 - r)) - 1.0

    h = W[15] if g == Grade.HARD else 1.0

    b = w[16] if g == Grade.EASY else 1.0

    c = exp(W[8])

    alpha = 1 + t_d*t_s*t_r*h*b*c

    return s*alpha
```

## Stability on Failure

The formula is different if the user selects `Forgot`:

$$
S'(D, S, R) = \min(S_f, S)
$$

$\min$ is there to ensure that stability at failure cannot be greater than $S$. $S_f$, stability on failure, is defined by:

$$
S_f = d_fs_fr_fc_f
$$

Where:

$$
\begin{align}
d_f &= D^{-w_{12}} \\
s_f &= ((S+1)^{w_{13}} - 1) \\
r_f &= e^{w_{14}(1-R)} \\
c_f &= w_{11} \\
\end{align}
$$

```{python}
def s_fail(d: D, s: S, r: R) -> S:
    d_f = D ** (-W[12])

    s_f = (S + 1.0) ** W[13] - 1.0

    r_s = exp(W[14] * (1.0 - r))

    c_f = W[11]

    S_f = d_f * s_f * r_f * c_f

    return min(S_f, s)
```

Putting it all together:

```{python}
def stability(d: D, s: S, r: R, g: Grade, first_time: bool) -> S:
    if first_time:
        return s_0(g)
    else:
        if g == Grade.FORGOT:
            return s_fail(d, s, r)
        else:
            return s_success(d, s, r, g)
```

# Updating Difficulty

This section describes how an item's difficulty is updated after a review.

## First Time

Analogously with stability: an item that has never been reviewed has no difficulty.

The initial difficulty, after the first review, is defined by:

$$
D_0(G) = w_4 - e^{w_5(G-1)} + 1
$$

```{python}
def d_0(g: Grade) -> D:
    return W[4] - exp(W[5] * (g.value - 1.0)) + 1
```

Note that when $G=1$ (forgot), then $D_0(1) = w_4$, that is, $w_4$ is the initial difficulty of an item when its first review was a failure.

```{python}
assert d_0(Grade.FORGOT) == W[4]
```

# Links

- Description of the algorithm: <https://github.com/open-spaced-repetition/fsrs4anki/wiki/The-Algorithm>
